# import libraries
import pandas as pd
import numpy as np
import spacy
from spacy import displacy
import networkx as nx
import os
import matplotlib.pyplot as plt
import scipy
import re


# Download English module

!python -m spacy download en_core_web_sm


# Load spacy English module

NER = spacy.load("en_core_web_sm")





# Load the text file

with open('KeyEvents20thCentury_article_Wiki.txt', 'r', errors='ignore') as file: 
   data = file.read().replace( '\n', ' ')


data








# Clean the text file to remove unnecessary elements
# Remove lines with navigation, menus, or tooltips
cleaned_text = re.sub(r'(\n\s*\n)+', '\n', data)  # Remove excessive newlines
cleaned_text = re.sub(r'\[edit\]', '', cleaned_text)   # Remove '[edit]' markers
cleaned_text = re.sub(r'[^A-Za-z0-9 .,\'\-\n]', '', cleaned_text)  # Remove special characters


cleaned_text_path = 'C:/Users/nrsmi/Documents/CareerFoundry/20th_Century/20thCentury_cleaned.txt'
with open(cleaned_text_path, 'w', encoding='utf-8') as cleaned_file:
    cleaned_file.write(cleaned_text)


# Load the text file

with open('20thCentury_cleaned.txt', 'r', errors='ignore') as file: 
   data_cleaned = file.read().replace( '\n', ' ')


data_cleaned








# Find all mentions of the United States and its variations in the cleaned text
us_variations = re.findall(r'\b(?:United States|America|USA|U\.S\.|US)\b', cleaned_text, re.IGNORECASE)


# Create a list of unique variations of the name
unique_us_variations = set(us_variations)
unique_us_variations


countries_list = pd.read_csv("countries_list.csv")
countries_list.head()


# Add individual alias columns directly
countries_list['Alias_1'] = None
countries_list['Alias_2'] = None
countries_list['Alias_3'] = None
countries_list['Alias_4'] = None


#add aliases for major countries

# Add aliases for the United States
countries_list.loc[countries_list['Country'] == 'United States', ['Alias_1', 'Alias_2', 'Alias_3', 'Alias_4']] = [
    'America', 'USA', 'U.S.', 'US']
# Update aliases for Germany and United Kingdom using the specified form
countries_list.loc[countries_list['Country'] == 'Germany', ['Alias_1', 'Alias_2', 'Alias_3', 'Alias_4']] = [
    'German', None, None, None]
countries_list.loc[countries_list['Country'] == 'United Kingdom', ['Alias_1', 'Alias_2', 'Alias_3', 'Alias_4']] = [
    'British', None, None, None]
# Update aliases for Italy
countries_list.loc[countries_list['Country'] == 'Italy', ['Alias_1', 'Alias_2', 'Alias_3', 'Alias_4']] = [
    'Italian', None, None, None]
# Update aliases for Russia
countries_list.loc[countries_list['Country'] == 'Russia', ['Alias_1', 'Alias_2', 'Alias_3', 'Alias_4']] = [
    'Soviet', 'USSR', None, None]
countries_list.tail(20)


# Save the updated DataFrame to a new file
final_updated_file_path = 'C:/Users/nrsmi/Documents/CareerFoundry/20th_Century/countries_list_with_aliases.csv'
countries_list.to_csv(final_updated_file_path, index=False)


countries_list2 = pd.read_csv("countries_list_with_aliases.csv")
countries_list2.head()


# remember most current txt file is "data_cleaned"


country_mentions = {}
for _, row in countries_list2.iterrows():
    country = row['Country']
    aliases = row[['Alias_1', 'Alias_2', 'Alias_3', 'Alias_4']].dropna().tolist()
    all_names = [country] + aliases
    count = 0
    for name in all_names:
        if name == "US":
            # Use regex to count "the US" but exclude "USSR"
            count += len(re.findall(r'\bthe US\b', data_cleaned))
        else:
            count += data_cleaned.lower().count(name.lower())  # Case-insensitive for other aliases
    if count > 0:
        country_mentions[country] = count


# Convert to DataFrame for better presentation
mentions_df = pd.DataFrame(list(country_mentions.items()), columns=['Country', 'Times mentioned']).sort_values(by='Times mentioned', ascending=False)
mentions_df


import seaborn as sns


plt.figure(figsize=(10, 15))
palette = sns.dark_palette("#79C", n_colors=len(mentions_df), reverse=False)
with sns.dark_palette("#79C", 27):
    sns.barplot(x = "Times mentioned", y = "Country", palette=palette,
    saturation=0.9, data = mentions_df.sort_values("Times mentioned", ascending = False)).set_title("Key Events 20th Century Wiki Article - most frequently mentioned countries")





data_cleaned = NER(data)


# Visualize identified entities

displacy.render(data_cleaned[273:20000], style = "ent", jupyter = True)





df_sentences = [] # empty shell to store results

# Loop through sentences, get entity list for each sentence
for sent in data_cleaned.sents:
    entity_list = [ent.text for ent in sent.ents]
    df_sentences.append({"sentence": sent, "entities": entity_list})
    
df_sentences = pd.DataFrame(df_sentences)


df_sentences.head(10)





# Function to filter out entities not of interest

def filter_entity(ent_list, countries_list2):
     # Combine all alias columns into a single list
    all_aliases = (
        list(countries_list2['Alias_1'].dropna()) +
        list(countries_list2['Alias_2'].dropna()) +
        list(countries_list2['Alias_3'].dropna()) +
        list(countries_list2['Alias_4'].dropna())
    )
    # Filter the entities
    return [ent for ent in ent_list if ent in all_aliases]


df_sentences['country_entities'] = df_sentences['entities'].apply(lambda x: filter_entity(x, countries_list2))


df_sentences['country_entities'].head(50)


# Filter out sentences that don't have any country entities

df_sentences_filtered = df_sentences[df_sentences['country_entities'].map(len) > 0]


df_sentences_filtered.tail(10)





# Create a mapping dictionary for aliases to main countries
alias_to_country = {}
for _, row in countries_list2.iterrows():
    main_country = row['Country']
    for alias in ['Alias_1', 'Alias_2', 'Alias_3', 'Alias_4']:
        if pd.notna(row[alias]):  # Check if the alias exists
            alias_to_country[row[alias]] = main_country

# Define relationships with a window size of 5
relationships = []  # Create an empty list

for i in range(df_sentences_filtered.index[-1]):
    end_i = min(i + 5, df_sentences_filtered.index[-1])
    cntry_list = sum((df_sentences_filtered.loc[i:end_i].country_entities), [])
    
    # Map aliases to main countries
    cntry_list = [alias_to_country.get(cntry, cntry) for cntry in cntry_list]
    
    # Remove duplicated countries that are next to each other
    cntry_unique = [cntry_list[i] for i in range(len(cntry_list)) 
                    if (i == 0) or cntry_list[i] != cntry_list[i - 1]]
    
    # Add relationships
    if len(cntry_unique) > 1:
        for idx, a in enumerate(cntry_unique[:-1]):
            b = cntry_unique[idx + 1]
            relationships.append({"source": a, "target": b})


relationship_df = pd.DataFrame(relationships)


relationship_df


# Sort the cases with a->b and b->a

relationship_df = pd.DataFrame(np.sort(relationship_df.values, axis = 1), columns = relationship_df.columns)
relationship_df.head(5)



# Summarize the interactions

relationship_df["value"] = 1
relationship_df = relationship_df.groupby(["source","target"], sort=False, as_index=False).sum()


relationship_df.head(10)


relationship_df.to_csv('C:/Users/nrsmi/Documents/CareerFoundry/20th_Century/countries_relationship.csv')



